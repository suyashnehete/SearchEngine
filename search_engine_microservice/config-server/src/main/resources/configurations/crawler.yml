server:
  port: ${CRAWLER_PORT:8082}

eureka:
  instance:
    hostname: localhost
  client:
    service-url:
      defaultZone: http://localhost:${EUREKA_PORT:8761}/eureka/

spring:
  application:
    name: crawler
  servlet:
    multipart:
      max-file-size: 50MB
  datasource:
    url: jdbc:postgresql://${DB_HOST:localhost}:${DB_PORT:5432}/${DB_NAME:search_engine}
    username: ${DB_USERNAME:admin}
    password: ${DB_PASSWORD:admin}
    driver-class-name: org.postgresql.Driver
    hikari:
      maximum-pool-size: 20
      minimum-idle: 5
      idle-timeout: 300000
      connection-timeout: 20000
      max-lifetime: 1200000
      leak-detection-threshold: 60000
      auto-commit: false
  jpa:
    hibernate:
      ddl-auto: update
    show-sql: false
    properties:
      hibernate:
        format_sql: true
        jdbc:
          batch_size: 25
          order_inserts: true
          order_updates: true
          lob:
            non_contextual_creation: true
        connection:
          provider_disables_autocommit: false
        cache:
          use_second_level_cache: false
          use_query_cache: false
    database: postgresql
    database-platform: org.hibernate.dialect.PostgreSQLDialect
  transaction:
    default-timeout: 30s
    rollback-on-commit-failure: true
  kafka:
    bootstrap-servers: ${KAFKA_BOOTSTRAP_SERVERS:localhost:9092}
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
      batch-size: ${KAFKA_PRODUCER_BATCH_SIZE:32768}
      linger-ms: ${KAFKA_PRODUCER_LINGER_MS:10}
      compression-type: ${KAFKA_PRODUCER_COMPRESSION:snappy}
    consumer:
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer
      group-id: crawler-service-group
      max-poll-records: ${KAFKA_CONSUMER_MAX_POLL_RECORDS:10}
      properties:
        spring.json.trusted.packages: "*"

redis:
  host: ${REDIS_HOST:localhost}
  port: ${REDIS_PORT:6379}
  timeout: 2000ms
  database: 1 # Crawler uses database 1
  jedis:
    pool:
      max-active: 8
      max-idle: 8
      min-idle: 0
      max-wait: -1ms

# Actuator configuration for health checks and metrics
management:
  endpoints:
    web:
      exposure:
        include: health,info,metrics,prometheus,circuitbreakers
  endpoint:
    health:
      show-details: always
      show-components: always
  health:
    circuitbreakers:
      enabled: true
    redis:
      enabled: true
  metrics:
    export:
      prometheus:
        enabled: true
    distribution:
      percentiles-histogram:
        http.server.requests: true
      percentiles:
        http.server.requests: 0.5, 0.95, 0.99

# Rate limiting configuration
crawler:
  rate-limit:
    requests-per-minute: 60
    requests-per-domain: 10

logging:
  level:
    com.searchengine.crawler: ${LOG_LEVEL:INFO}
    org.springframework.kafka: ${LOG_LEVEL:INFO}
    org.hibernate.SQL: ${LOG_LEVEL:WARN}
