package com.suyash.se.configserver;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cloud.config.server.EnableConfigServer;

@EnableConfigServer
@SpringBootApplication
public class ConfigServerApplication {

	public static void main(String[] args) {
		SpringApplication.run(ConfigServerApplication.class, args);
	}

}


eureka:
    instance:
        hostname: localhost
    client:
        serviceUrl:
            defaultZone: http://localhost:8761/eureka/

server:
    port: 8090
    servlet:
        context-path: /api/v1

spring:
    application:
        name: crawler
    servlet:
        multipart:
            max-file-size: 50MB
    datasource:
        url: jdbc:postgresql://localhost:5432/search_engine
        username: admin
        password: admin
        driver-class-name: org.postgresql.Driver
    jpa:
        hibernate:
            ddl-auto: update
        show-sql: false
        properties:
            hibernate:
                format_sql: true
        database: postgresql
        database-platform: org.hibernate.dialect.PostgreSQLDialect

application:
    config:
        indexer:
            url: http://localhost:8222/api/v1/indexer

eureka:
    instance:
        hostname: localhost
    client:
        registerWithEureka: false
        fetchRegistry: false
        serviceUrl:
            defaultZone: http://${eureka.instance.hostname}:${server.port}/eureka/

server:
    port: 8761

spring:
    application:
        name: discovery


eureka:
  client:
    service-url:
      defaultZone: http://localhost:8761/eureka
    register-with-eureka: false
    fetch-registry: false
  instance:
    hostname: localhost

spring:
    application:
        name: gateway
    web:
        resources:
            add-mappings: false
    cloud:
        gateway:
            mvc:
                discovery:
                    locator:
                        enabled: true
                routes:
                    -   id: crawler
                        uri: http://localhost:8090
                        predicates:
                            -   Path=/api/v1/crawler/**
                    -   id: indexer
                        uri: http://localhost:8091
                        predicates:
                            -   Path=/api/v1/indexer/**
                    -   id: query
                        uri: http://localhost:8092
                        predicates:
                            -   Path=/api/v1/search/**, /api/v1/suggestions/**, /api/v1/log/**

server:
    port: 8222

eureka:
    instance:
        hostname: localhost
    client:
        serviceUrl:
            defaultZone: http://localhost:8761/eureka/

server:
    port: 8091
    servlet:
        context-path: /api/v1

spring:
    application:
        name: indexer
    servlet:
        multipart:
            max-file-size: 50MB
    datasource:
        url: jdbc:postgresql://localhost:5432/search_engine
        username: admin
        password: admin
        driver-class-name: org.postgresql.Driver
    jpa:
        hibernate:
            ddl-auto: update
        show-sql: false
        properties:
            hibernate:
                format_sql: true
        database: postgresql
        database-platform: org.hibernate.dialect.PostgreSQLDialect

application:
    config:
        crawler:
            url: http://localhost:8222/api/v1/crawler

eureka:
    instance:
        hostname: localhost
    client:
        serviceUrl:
            defaultZone: http://localhost:8761/eureka/

server:
    port: 8092
    servlet:
        context-path: /api/v1

spring:
    application:
        name: query
    servlet:
        multipart:
            max-file-size: 50MB
    datasource:
        url: jdbc:postgresql://localhost:5432/search_engine
        username: admin
        password: admin
        driver-class-name: org.postgresql.Driver
    jpa:
        hibernate:
            ddl-auto: update
        show-sql: false
        properties:
            hibernate:
                format_sql: true
        database: postgresql
        database-platform: org.hibernate.dialect.PostgreSQLDialect

application:
    config:
        indexer:
            url: http://localhost:8222/api/v1/indexer
        crawler:
            url: http://localhost:8222/api/v1/crawler

server:
  port: 8888
  
spring:
    profiles:
        active: native
    application:
        name: config-server
    cloud:
        config:
            server:
                native:
                    search-locations: classpath:/configurations

@SpringBootApplication
public class GatewayApplication {

	public static void main(String[] args) {
		SpringApplication.run(GatewayApplication.class, args);
	}

}

@Configuration
@AllArgsConstructor
public class BeansConfig {

    @Bean
    public CorsFilter corsFilter() {
        final UrlBasedCorsConfigurationSource source = new UrlBasedCorsConfigurationSource();
        final CorsConfiguration config = new CorsConfiguration();
        config.setAllowCredentials(true);
        config.setAllowedOrigins(Collections.singletonList("http://localhost:4200"));
        config.setAllowedHeaders(Arrays.asList(
                HttpHeaders.ORIGIN,
                HttpHeaders.ACCEPT,
                HttpHeaders.CONTENT_TYPE,
                HttpHeaders.AUTHORIZATION
        ));
        config.setAllowedMethods(Arrays.asList("GET", "POST", "PUT", "DELETE", "OPTIONS"));
        source.registerCorsConfiguration("/**", config);
        return new CorsFilter(source);
    }
}

@SpringBootApplication
@EnableFeignClients
@EnableDiscoveryClient
@EnableJpaAuditing
public class QueryApplication {

	public static void main(String[] args) {
		SpringApplication.run(QueryApplication.class, args);
	}

}

@RestController
@RequiredArgsConstructor
@RequestMapping("suggestions")
public class SuggestionsController {
    private final QueryService queryService;

    @GetMapping("trie")
    public List<String> getSuggestionsTrie(@RequestParam String prefix) {
        return queryService.getSuggestionsTrie(prefix);
    }

    @GetMapping("ngram")
    public List<String> getSuggestionsNGram(@RequestParam String prefix) {
        return queryService.getSuggestionsNGram(prefix);
    }

    @GetMapping
    public List<String> getSuggestions(@RequestParam String userId, @RequestParam String prefix) {
        return queryService.getContextAwareSuggestions(userId, prefix);
    }
}

public class EditDistance {
    public static int calculate(String s1, String s2) {
        int m = s1.length();
        int n = s2.length();

        // Create a DP table
        int[][] dp = new int[m + 1][n + 1];

        // Initialize the table
        for (int i = 0; i <= m; i++) {
            dp[i][0] = i;
        }
        for (int j = 0; j <= n; j++) {
            dp[0][j] = j;
        }

        // Fill the table
        for (int i = 1; i <= m; i++) {
            for (int j = 1; j <= n; j++) {
                if (s1.charAt(i - 1) == s2.charAt(j - 1)) {
                    dp[i][j] = dp[i - 1][j - 1]; // No operation needed
                } else {
                    dp[i][j] = 1 + Math.min(dp[i - 1][j - 1], // Substitution
                            Math.min(dp[i - 1][j], // Deletion
                                    dp[i][j - 1])); // Insertion
                }
            }
        }

        return dp[m][n];
    }
}

public class NGramModel {
    private final Map<String, Map<String, Integer>> nGramMap = new HashMap<>();
    private final int n;

    public NGramModel(int n) {
        this.n = n;
    }

    public void train(List<String> queries) {
        for (String query : queries) {
            List<String> tokens = tokenize(query);
            for (int i = 0; i <= tokens.size() - n; i++) {
                String prefix = String.join(" ", tokens.subList(i, i + n - 1));
                String nextWord = tokens.get(i + n - 1);

                nGramMap.computeIfAbsent(prefix, k -> new HashMap<>()).merge(nextWord, 1, Integer::sum);
            }
        }
    }

    public List<String> getSuggestions(String prefix) {
        Map<String, Integer> suggestions = nGramMap.getOrDefault(prefix, new HashMap<>());
        return suggestions.entrySet().stream()
                .sorted(Map.Entry.<String, Integer>comparingByValue().reversed())
                .map(Map.Entry::getKey)
                .toList();
    }

    private List<String> tokenize(String text) {
        return Arrays.asList(text.toLowerCase().split("\\s+"));
    }
}

@Builder
public record UrlResponse(
        long documentId,
        String url,
        String title,
        String shortContent
) {

}
Builder
public record SearchResponse(
        List<UrlResponse> documents,
        int totalResults,
        int totalPages,
        int currentPage,
        int pageSize
) {

}
@RequiredArgsConstructor
@Service
public class QueryService {

    private final CrawlerClient crawlerClient;
    private final IndexerClient indexerClient;
    private final LRUCacheService<String, List<Integer>> cacheService;

    private static final Pattern WORD_PATTERN = Pattern.compile("\\w+");
    private static final Set<String> STOP_WORDS = Set.of("the", "and", "is", "in", "to", "of", "a", "for");
    
    private Trie queryTrie = new Trie();
    private NGramModel nGramModel = new NGramModel(2);
    private Map<String, List<String>> userSearchHistory = new HashMap<>();
    private AtomicInteger queryCount = new AtomicInteger(0);
    private AtomicInteger cacheHitCount = new AtomicInteger(0);

    // Populate Trie with frequent queries from cache
    @PostConstruct
    public void populateTrie() {
        cacheService.asMap().keySet().forEach(queryTrie::insert);
    }

    @PostConstruct
    public void trainNGramModel() {
        List<String> frequentQueries = new ArrayList<>(cacheService.asMap().keySet());
        nGramModel.train(frequentQueries);
    }

    public void logUserQuery(String userId, String query) {
        userSearchHistory.computeIfAbsent(userId, k -> new ArrayList<>()).add(query);
    }

    public List<String> getContextAwareSuggestions(String userId, String prefix) {
        List<String> history = userSearchHistory.getOrDefault(userId, Collections.emptyList());
        Map<String, Integer> contextScores = new HashMap<>();

        // Prioritize suggestions based on user history
        for (String pastQuery : history) {
            if (pastQuery.startsWith(prefix)) {
                contextScores.put(pastQuery, contextScores.getOrDefault(pastQuery, 0) + 5); // Higher weight for history
            }
        }

        // Combine with N-gram suggestions
        List<String> nGramSuggestions = nGramModel.getSuggestions(prefix);
        for (String suggestion : nGramSuggestions) {
            contextScores.put(suggestion, contextScores.getOrDefault(suggestion, 0) + 1);
        }

        // Return top suggestions based on context scores
        return contextScores.entrySet().stream()
                .sorted(Map.Entry.<String, Integer>comparingByValue().reversed())
                .map(Map.Entry::getKey)
                .limit(5) // Limit to top 5 suggestions
                .toList();
    }

    public List<Integer> processQuery(String query, int topK) {

        queryCount.incrementAndGet();

        // Check cache first
        List<Integer> cachedResults = cacheService.getIfPresent(query);
        if (cachedResults != null) {
            System.out.println("Cache hit for query: " + query);
            cacheHitCount.incrementAndGet();
            return cachedResults;
        }

        // Tokenize and normalize the query
        String[] queryTerms = tokenize(query);

        // Retrieve document IDs for each term
        Map<String, Set<Integer>> termToDocIds = new HashMap<>();
        for (String term : queryTerms) {
            if (!STOP_WORDS.contains(term)) {
                InvertedIndex index = indexerClient.findByWord(term);
                if (index != null) {
                    termToDocIds.put(term, new HashSet<>(index.getDocumentIds()));
                }
            }
        }

        // Combine results using Boolean AND logic
        Set<Integer> resultDocIds = null;
        for (Set<Integer> docIds : termToDocIds.values()) {
            if (resultDocIds == null) {
                resultDocIds = new HashSet<>(docIds);
            } else {
                resultDocIds.retainAll(docIds); // Intersection of sets
            }
        }

        // Rank documents by relevance (simple frequency-based ranking)
        if (resultDocIds == null || resultDocIds.isEmpty()) {
            return Collections.emptyList();
        }

        // Rank documents by relevance
        List<Integer> rankedResults = rankDocuments(resultDocIds, termToDocIds, topK);

        // Store results in cache
        cacheService.put(query, rankedResults);

        return rankedResults;
    }

    public SearchResponse processQuery(String query, int topK, int page, int size) {
        List<Integer> allResults = processQueryWithRanking(query, topK);
        int start = (page - 1) * size;
        int end = Math.min(start + size, allResults.size());
        List<Integer> pagedResults = Collections.emptyList();
        if (start <= end) {
            pagedResults = allResults.subList(start, end);
        }

        List<UrlResponse> urlResponses = pagedResults.stream()
                .map(docId -> {
                    CrawledPage pageObj = crawlerClient.findById((long) docId).orElse(null);
                    if (pageObj == null) {
                        return null;
                    }
                    return UrlResponse.builder()
                            .documentId(pageObj.getId())
                            .title(pageObj.getTitle())
                            .url(pageObj.getUrl())
                            .shortContent(pageObj.getShortContent())
                            .build();
                })
                .filter(Objects::nonNull)
                .toList();

        return SearchResponse.builder()
                .documents(urlResponses)
                .totalResults(allResults.size())
                .totalPages((int) Math.ceil((double) allResults.size() / size))
                .currentPage(page)
                .pageSize(size)
                .build();
    }

    public SearchResponse processMultipleQuery(List<String> query, int topK, int page, int size) {
        List<Integer> allResults = new ArrayList<>();
        for (String q : query) {
            allResults.addAll(processQueryWithRanking(q, topK / query.size()));
        }
        int start = (page - 1) * size;
        int end = Math.min(start + size, allResults.size());
        List<Integer> pagedResults = Collections.emptyList();
        if (start <= end) {
            pagedResults = allResults.subList(start, end);
        }

        List<UrlResponse> urlResponses = pagedResults.stream()
                .map(docId -> {
                    CrawledPage pageObj = crawlerClient.findById((long) docId).orElse(null);
                    if (pageObj == null) {
                        return null;
                    }
                    return UrlResponse.builder()
                            .documentId(pageObj.getId())
                            .title(pageObj.getTitle())
                            .url(pageObj.getUrl())
                            .shortContent(pageObj.getShortContent())
                            .build();
                })
                .filter(Objects::nonNull)
                .toList();

        return SearchResponse.builder()
                .documents(urlResponses)
                .totalResults(allResults.size())
                .totalPages((int) Math.ceil((double) allResults.size() / size))
                .currentPage(page)
                .pageSize(size)
                .build();
    }

    public List<Integer> processQueryWithRanking(String query, int topK) {

        queryCount.incrementAndGet();

        // Check cache first
        List<Integer> cachedResults = cacheService.getIfPresent(query);
        if (cachedResults != null) {
            System.out.println("Cache hit for query: " + query);
            cacheHitCount.incrementAndGet();
            return cachedResults;
        }

        // Tokenize and normalize the query
        String[] queryTerms = tokenize(query);

        // Retrieve document IDs for each term
        Map<String, Set<Integer>> termToDocIds = new HashMap<>();
        Map<Integer, Double> docScores = new HashMap<>();

        for (String term : queryTerms) {
            InvertedIndex index = indexerClient.findByWord(term);
            if (index != null) {
                termToDocIds.put(term, new HashSet<>(index.getDocumentIds()));

                // Calculate frequency-based scores
                for (Integer docId : index.getDocumentIds()) {
                    CrawledPage page = crawlerClient.findById((long) docId).orElse(null);
                    if (page != null) {
                        double positionScore = calculatePositionScore(page.getContent(), term);
                        docScores.put(docId, docScores.getOrDefault(docId, 0.0) + positionScore);
                    }
                }
            }
        }

        // Combine results using Boolean AND logic
        Set<Integer> resultDocIds = null;
        for (Set<Integer> docIds : termToDocIds.values()) {
            if (resultDocIds == null) {
                resultDocIds = new HashSet<>(docIds);
            } else {
                resultDocIds.retainAll(docIds); // Intersection of sets
            }
        }

        // Rank documents by frequency-based scores
        List<Integer> rankedResults = frequencyRankedDocuments(resultDocIds, docScores, topK);
        // Store results in cache
        cacheService.put(query, rankedResults);
        return rankedResults;
    }

    private int countOccurrences(String content, String term) {
        int count = 0;
        int index = 0;
        while ((index = content.indexOf(term, index)) != -1) {
            count++;
            index += term.length();
        }
        return count;
    }

    private List<Integer> frequencyRankedDocuments(Set<Integer> docIds, Map<Integer, Double> docScores, int topK) {
        if (docIds == null) {
            return Collections.emptyList();
        }
        return docIds.stream()
                .sorted((id1, id2) -> {
                    double score1 = docScores.getOrDefault(id1, 0.0)
                            + crawlerClient.findById((long) id1).orElse(null).getPageRankScore();
                    double score2 = docScores.getOrDefault(id2, 0.0)
                            + crawlerClient.findById((long) id2).orElse(null).getPageRankScore();
                    return Double.compare(score2, score1);
                })
                .limit(topK)
                .toList();
    }

    private String[] tokenize(String text) {
        return WORD_PATTERN.matcher(text.toLowerCase()).results()
                .map(match -> match.group())
                .toArray(String[]::new);
    }

    private double calculatePositionScore(String content, String term) {
        int index = content.toLowerCase().indexOf(term.toLowerCase());
        if (index == -1) {
            return 0.0; // Term not found
        }
        // Higher score for terms appearing earlier in the content
        return 1.0 / (index + 1);
    }

    private List<Integer> rankDocuments(Set<Integer> docIds, Map<String, Set<Integer>> termToDocIds, int topK) {
        // Simple ranking based on term frequency
        Map<Integer, Integer> docScores = new HashMap<>();
        for (Integer docId : docIds) {
            int score = 0;
            for (Set<Integer> docList : termToDocIds.values()) {
                if (docList.contains(docId)) {
                    score++;
                }
            }
            docScores.put(docId, score);
        }

        // Sort documents by score in descending order
        return docScores.entrySet().stream()
                .sorted(Map.Entry.<Integer, Integer>comparingByValue().reversed())
                .limit(topK)
                .map(Map.Entry::getKey)
                .toList();
    }

    public List<String> getSuggestionsTrie(String prefix) {
        return queryTrie.getSuggestions(prefix);
    }

    public List<String> getSuggestionsNGram(String prefix) {
        return nGramModel.getSuggestions(prefix);
    }

    public List<Integer> processQueryWithFilters(String query, List<String> tags, int topK) {
        List<Integer> results = processQueryWithRanking(query, topK);

        if (tags == null || tags.isEmpty()) {
            return results;
        }

        return results.stream()
                .filter(docId -> {
                    CrawledPage page = crawlerClient.findById((long) docId).orElse(null);
                    return page != null && page.getTags().containsAll(tags);
                })
                .toList();
    }

    public int getQueryCount() {
        return queryCount.get();
    }

    public double getCacheHitRate() {
        return queryCount.get() == 0 ? 0 : (double) cacheHitCount.get() / queryCount.get();
    }

    public SearchResponse processQueryWithCorrections(String query, int topK, int page, int size) {
        Set<String> allQueries = new HashSet<>();
        allQueries.add(query);
        allQueries.addAll(getContextAwareSuggestions("anonymous", query));
        allQueries.addAll(getSuggestionsNGram(query));
        allQueries.addAll(getSuggestionsTrie(query));
        SearchResponse results = processQuery(query, topK, page, size);

        if (!results.documents().isEmpty()) {
            return results;
        }

        return suggestCorrections(query, topK, page, size);
    }

    private SearchResponse suggestCorrections(String query, int topK, int page, int size) {
        Set<String> allQueries = new HashSet<>(cacheService.asMap().keySet());
        Map<String, Integer> corrections = new HashMap<>();
        for (String cachedQuery : allQueries) {
            int distance = EditDistance.calculate(query, cachedQuery);
            if (distance <= query.length() / 3) {
                corrections.put(cachedQuery, distance);
            }
        }

        allQueries.addAll(getContextAwareSuggestions("anonymous", query));
        allQueries.addAll(getSuggestionsNGram(query));
        allQueries.addAll(getSuggestionsTrie(query));

        List<String> topQueries = corrections.entrySet().stream()
                .sorted(Map.Entry.<String, Integer>comparingByValue())
                .limit(topK)
                .map(entry -> {
                    return entry.getKey();
                })
                .toList();
        return processMultipleQuery(topQueries, topK, page, size);
    }

}

@RequiredArgsConstructor
@RestController
public class QueryController {

    private final QueryService queryService;

    @GetMapping("/search")
    public SearchResponse search(
            @RequestParam String query,
            @RequestParam(defaultValue = "10") int topK,
            @RequestParam(defaultValue = "1") int page,
            @RequestParam(defaultValue = "10") int size) {
        return queryService.processQueryWithCorrections(query, topK, page, size);
    }
}

@RestController
@RequiredArgsConstructor
@RequestMapping("log")
public class QueryLoggingController {

    private final QueryService queryService;

    @PostMapping("log-query")
    public ResponseEntity<?> logQuery(@RequestBody Map<String, String> requestBody) {
        String userId = requestBody.get("userId");
        String query = requestBody.get("query");

        if (userId == null || query == null) {
            return ResponseEntity.badRequest().build();
        }

        queryService.logUserQuery(userId, query);
        return ResponseEntity.ok().build();
    }
}

@FeignClient(name = "indexer", url = "${application.config.indexer.url}")
public interface IndexerClient {

    @PostMapping("find")
    InvertedIndex findByWord(@RequestBody String term);

    
}
@Getter
@Setter
@Builder
@AllArgsConstructor
@NoArgsConstructor
public class InvertedIndex {
    private Long id;

    private String word;

    private List<Integer> documentIds;

    private Map<Integer, Double> tfidfScores;
}

@Getter
@Setter
@AllArgsConstructor
@NoArgsConstructor
@Builder
public class CrawledPage {

    private Long id;

    private String url;

    private String title;

    private String shortContent;

    private String content;

    private LocalDateTime createdDate;

    private List<String> tags;

    private double pageRankScore;
}


@FeignClient(name = "crawler", url = "${application.config.crawler.url}")
public interface CrawlerClient {

    @GetMapping("/findById/{id}")
    Optional<CrawledPage> findById(@PathVariable("id") long id);

}


public class LRUCache<K, V> {
    private final int capacity;
    private final Map<K, Node<K, V>> map;
    private final DoublyLinkedList<K, V> list;
    private final long ttl; // Time to live in minutes
    private final ExpirationPolicy expirationPolicy;
    private final ScheduledExecutorService scheduler;
    private final ReentrantLock lock = new ReentrantLock();

    public LRUCache(int capacity, long ttlInMinutes, ExpirationPolicy policy) {
        this.capacity = capacity;
        this.map = new ConcurrentHashMap<>();
        this.list = new DoublyLinkedList<>();
        this.ttl = ttlInMinutes * 60 * 1000; // Convert minutes to milliseconds
        this.expirationPolicy = policy;

        // Schedule a background cleanup task to run every 5 minutes
        this.scheduler = Executors.newScheduledThreadPool(1);
        this.scheduler.scheduleAtFixedRate(this::cleanupExpiredEntries, 5, 5, TimeUnit.MINUTES);
    }

    public V get(K key) {
        lock.lock();
        try {
            if (!map.containsKey(key)) {
                return null;
            }
            Node<K, V> node = map.get(key);

            // Update timestamp for AFTER_ACCESS policy
            if (expirationPolicy == ExpirationPolicy.AFTER_ACCESS) {
                node.timestamp = System.currentTimeMillis();
            }

            // Check if the entry has expired
            if (isExpired(node)) {
                remove(key);
                return null;
            }

            list.moveToFront(node);
            return node.value;
        } finally {
            lock.unlock();
        }
    }

    public void put(K key, V value) {
        lock.lock();
        try {
            if (map.containsKey(key)) {
                Node<K, V> node = map.get(key);
                node.value = value;
                node.timestamp = System.currentTimeMillis(); // Update timestamp
                list.moveToFront(node);
            } else {
                if (map.size() == capacity) {
                    Node<K, V> removedNode = list.removeLast();
                    map.remove(removedNode.key);
                }
                Node<K, V> newNode = new Node<>(key, value, System.currentTimeMillis());
                map.put(key, newNode);
                list.addFirst(newNode);
            }
        } finally {
            lock.unlock();
        }
    }

    private boolean isExpired(Node<K, V> node) {
        return System.currentTimeMillis() - node.timestamp > ttl;
    }

    private void remove(K key) {
        lock.lock();
        try {
            Node<K, V> node = map.remove(key);
            if (node != null) {
                list.remove(node);
            }
        } finally {
            lock.unlock();
        }
    }

    private void cleanupExpiredEntries() {
        lock.lock();
        try {
            Iterator<Map.Entry<K, Node<K, V>>> iterator = map.entrySet().iterator();
            while (iterator.hasNext()) {
                Map.Entry<K, Node<K, V>> entry = iterator.next();
                if (isExpired(entry.getValue())) {
                    iterator.remove();
                    list.remove(entry.getValue());
                }
            }
        } finally {
            lock.unlock();
        }
    }

    public void shutdown() {
        scheduler.shutdown();
    }

    public Map<K, V> getAllCache() {
        lock.lock();
        try {
            Map<K, V> cache = new LinkedHashMap<>();

            for (Map.Entry<K, Node<K, V>> mapEntry : map.entrySet()) {
                K key = mapEntry.getKey();
                V value = get(key);

                cache.put(key, value);
            }
            return cache;
        } finally {
            lock.unlock();
        }
    }

    public enum ExpirationPolicy {
        AFTER_WRITE,
        AFTER_ACCESS
    }

    private static class Node<K, V> {
        K key;
        V value;
        Node<K, V> prev;
        Node<K, V> next;
        long timestamp; // Timestamp when the node was created or updated

        Node(K key, V value, long timestamp) {
            this.key = key;
            this.value = value;
            this.timestamp = timestamp;
        }
    }

    private static class DoublyLinkedList<K, V> {
        private Node<K, V> head;
        private Node<K, V> tail;

        public DoublyLinkedList() {
            head = new Node<>(null, null, 0);
            tail = new Node<>(null, null, 0);
            head.next = tail;
            tail.prev = head;
        }

        public void addFirst(Node<K, V> node) {
            node.next = head.next;
            node.prev = head;
            head.next.prev = node;
            head.next = node;
        }

        public void moveToFront(Node<K, V> node) {
            remove(node);
            addFirst(node);
        }

        public Node<K, V> removeLast() {
            if (tail.prev == head) {
                return null;
            }
            return remove(tail.prev);
        }

        public Node<K, V> remove(Node<K, V> node) {
            node.prev.next = node.next;
            node.next.prev = node.prev;
            node.prev = null;
            node.next = null;
            return node;
        }
    }
}



@Service
public class LRUCacheService<K, V> {
    final int CAPACITY = 1000;
    private final LRUCache<K, V> cache = new LRUCache<>(CAPACITY, 10, LRUCache.ExpirationPolicy.AFTER_ACCESS);

    public V getIfPresent(K key) {
        return cache.get(key);
    }

    public void put(K key, V value) {
        cache.put(key, value);
    }

    public Map<K, V> asMap() {
        return cache.getAllCache();
    }
}

package com.suyash.se.crawler;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cloud.client.discovery.EnableDiscoveryClient;
import org.springframework.cloud.openfeign.EnableFeignClients;
import org.springframework.data.jpa.repository.config.EnableJpaAuditing;

@EnableFeignClients
@SpringBootApplication
@EnableJpaAuditing
@EnableDiscoveryClient
public class CrawlerApplication {

	public static void main(String[] args) {
		SpringApplication.run(CrawlerApplication.class, args);
	}

}

package com.suyash.se.crawler.indexer;

import java.util.List;

import org.springframework.cloud.openfeign.FeignClient;
import org.springframework.web.bind.annotation.PostMapping;
import org.springframework.web.bind.annotation.RequestBody;

import com.suyash.se.crawler.crawler.CrawledPage;

@FeignClient(name = "indexer", url = "${application.config.indexer.url}")
public interface IndexerClient {

    @PostMapping
    Boolean buildIndex(@RequestBody List<CrawledPage> pages);
}

@Getter
@Setter
@AllArgsConstructor
@NoArgsConstructor
@Builder
@Entity(name = "crawled_pages")
@EntityListeners(AuditingEntityListener.class)
public class CrawledPage {

    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @Column(nullable = false, unique = true)
    private String url;

    @Column(nullable = false)
    private String title;

    @Column(columnDefinition = "TEXT")
    private String shortContent;

    @Lob
    private String content;

    @CreatedDate
    @Column(nullable = false, updatable = false)
    private LocalDateTime createdDate;

    @ElementCollection
    @CollectionTable(name = "page_tags", joinColumns = @JoinColumn(name = "page_id"))
    private List<String> tags;

    @Column(nullable = false)
    private double pageRankScore;
}


public interface CrawledPageRepository extends JpaRepository<CrawledPage, Long> {
    boolean existsByUrl(String url);

    CrawledPage findByUrl(String nextUrl);
}

@RestController
@RequiredArgsConstructor
@RequestMapping("crawler")
public class CrawlerController {

    private final WebCrawlerService webCrawlerService;
    private final CrawledPageRepository crawledPageRepository;

    @PostMapping()
    public ResponseEntity<?> submitUrl(@RequestBody Map<String, String> requestBody) {
        String url = requestBody.get("url");
        if (url == null || !webCrawlerService.isValidUrl(url)) {
            return ResponseEntity.badRequest().body("Invalid URL");
        }

        webCrawlerService.addUrlToQueue(url);
        Map<String, String> response = new HashMap<>();
        response.put("message", "URL added to queue");
        response.put("url", url);
        return ResponseEntity.ok(response);
    }

    @PostMapping("/save")
    public ResponseEntity<Boolean> save(@RequestBody CrawledPage page) {
        crawledPageRepository.save(page);
        return ResponseEntity.ok(true);
    }

    @PostMapping("/findByUrl")
    public ResponseEntity<CrawledPage> findByUrl(@RequestBody String url) {
        return ResponseEntity.ok(crawledPageRepository.findByUrl(url));
    }

    @GetMapping("/findById/{id}")
    public ResponseEntity<CrawledPage> findById(@PathVariable(name = "id") long key) {
        return ResponseEntity.ok(crawledPageRepository.findById(key).orElse(null));
    }
}

@RequiredArgsConstructor
@Service
public class WebCrawlerService {

    private final CrawledPageRepository repository;
    private final IndexerClient indexerClient;

    private Queue<String> queue = new ConcurrentLinkedQueue<>();
    private HashSet<String> visited = new HashSet<>();
    private boolean isCrawling = false;

    public void addUrlToQueue(String url) {
        if (isValidUrl(url) && !visited.contains(url)) {
            queue.add(url);
            System.out.println("Added URL to queue: " + url);
        }
        if (!isCrawling) {
            startCrawling();
        }
    }

    public void startCrawling() {
        new Thread(() -> {
            isCrawling = true;
            List<CrawledPage> pages = new ArrayList<>();
            while (!queue.isEmpty() && visited.size() < 100) { // Limiting to 100 pages for now
                String currentUrl = queue.poll();
                if (currentUrl == null) {
                    try {
                        Thread.sleep(5000); // Wait for 5 seconds before checking again
                    } catch (InterruptedException e) {
                        Thread.currentThread().interrupt();
                    }
                    continue;
                }

                if (!visited.contains(currentUrl)) {
                    visited.add(currentUrl);
                    CrawledPage page = crawl(currentUrl);
                    if (page != null) {
                        pages.add(page);
                    }
                }
            }
            buildIndex(pages);
            isCrawling = false;
        }).start();
    }

    private void buildIndex(List<CrawledPage> pages) {
       indexerClient.buildIndex(pages);
    }

    private CrawledPage crawl(String currentUrl) {
        System.out.println("Crawling: " + currentUrl);

        try {
            Document doc = Jsoup.connect(currentUrl).get();
            // Extract title
            String title = doc.title();

            // Extract short content (first 200 characters of the text)
            String text = doc.body().text();
            String shortContent = text.length() > 200 ? text.substring(0, 200) + "..." : text;

            CrawledPage page = saveToDatabase(currentUrl, title, shortContent, text);

            Elements links = doc.select("a[href]");
            for (Element link : links) {
                String nextUrl = link.absUrl("href");
                if (!visited.contains(nextUrl) && isValidUrl(nextUrl)) {
                    queue.add(nextUrl);
                }
            }
            return page;
        } catch (Exception e) {
            System.err.println("Error crawling " + currentUrl + ": " + e.getMessage());
        }
        return null;
    }

    private CrawledPage saveToDatabase(String url, String title, String shortContent, String text) {
        CrawledPage page = CrawledPage.builder()
                .url(url)
                .content(text)
                .shortContent(shortContent)
                .title(title)
                .build();
        repository.save(page);
        System.out.println("Saved to Database - URL: " + url);
        return page;
    }

    public boolean isValidUrl(String url) {
        return url != null && (url.startsWith("http://") || url.startsWith("https://"));
    }

}


@EnableEurekaServer
@SpringBootApplication
public class DiscoveryApplication {

	public static void main(String[] args) {
		SpringApplication.run(DiscoveryApplication.class, args);
	}

}


@EnableFeignClients
@SpringBootApplication
@EnableJpaAuditing
@EnableDiscoveryClient
public class IndexerApplication {

	public static void main(String[] args) {
		SpringApplication.run(IndexerApplication.class, args);
	}

}

public class PageRank {
    private static final double DAMPING_FACTOR = 0.85;
    private static final int MAX_ITERATIONS = 100;
    private static final double CONVERGENCE_THRESHOLD = 0.001;

    public static Map<Integer, Double> calculate(Map<Integer, List<Integer>> adjacencyList) {
        int numNodes = adjacencyList.size();
        Map<Integer, Double> pageRank = new HashMap<>();
        Map<Integer, List<Integer>> outDegreeMap = new HashMap<>();

        // Initialize PageRank scores and calculate out-degrees
        for (int node : adjacencyList.keySet()) {
            pageRank.put(node, 1.0 / numNodes);
            outDegreeMap.put(node, adjacencyList.getOrDefault(node, Collections.emptyList()));
        }

        // Iteratively update PageRank scores
        for (int iteration = 0; iteration < MAX_ITERATIONS; iteration++) {
            Map<Integer, Double> newPageRank = new HashMap<>();
            double totalDiff = 0.0;

            for (int node : adjacencyList.keySet()) {
                double score = (1 - DAMPING_FACTOR) / numNodes;

                for (int incomingNode : adjacencyList.getOrDefault(node, Collections.emptyList())) {
                    int outDegree = outDegreeMap.get(incomingNode).size();
                    if (outDegree > 0) {
                        score += DAMPING_FACTOR * (pageRank.get(incomingNode) / outDegree);
                    }
                }

                newPageRank.put(node, score);
                totalDiff += Math.abs(score - pageRank.get(node));
            }

            pageRank = newPageRank;

            // Check for convergence
            if (totalDiff < CONVERGENCE_THRESHOLD) {
                break;
            }
        }

        return pageRank;
    }
}

@RestController
@RequestMapping("indexer")
@RequiredArgsConstructor
public class IndexerController {

    private final IndexerService indexerService;
    private final InvertedIndexRepository invertedIndexRepository;

    @PostMapping
    public ResponseEntity<Boolean> buildIndex(@RequestBody List<CrawledPage> pages) {
        indexerService.buildIndex(pages);
        return ResponseEntity.ok(true);
    }

    @PostMapping("find")
    public ResponseEntity<InvertedIndex> findByWord(@RequestBody String term) {
        return ResponseEntity.ok(invertedIndexRepository.findByWord(term));
    }

}

@RequiredArgsConstructor
@Service
public class IndexerService {

    private static final Pattern WORD_PATTERN = Pattern.compile("\\w+");
    private static final Set<String> STOP_WORDS = Set.of("the", "and", "is", "in", "to", "of", "a", "for");
    private final InvertedIndexRepository invertedIndexRepository;
    private final CrawlerClient crawlerClient;

    public void buildIndex(List<CrawledPage> pages) {
        Map<String, Map<Integer, Integer>> termFrequencyMap = new HashMap<>();
        Map<String, Integer> documentFrequencyMap = new HashMap<>();

        // Build adjacency list for links between pages
        Map<Integer, List<Integer>> adjacencyList = new HashMap<>();
        for (CrawledPage page : pages) {
            int docId = Math.toIntExact(page.getId());
            String content = page.getContent();

            // Tokenize and normalize the text
            String[] words = tokenize(content);

            Set<String> uniqueWords = new HashSet<>();
            for (String word : words) {
                if (!STOP_WORDS.contains(word)) {
                    uniqueWords.add(word);

                    // Update term frequency
                    termFrequencyMap.computeIfAbsent(word, k -> new HashMap<>())
                            .merge(docId, 1, Integer::sum);
                }
            }

            // Update document frequency
            for (String word : uniqueWords) {
                documentFrequencyMap.merge(word, 1, Integer::sum);
            }

            // Extract links and build adjacency list
            Document doc = Jsoup.parse(content);
            Elements links = doc.select("a[href]");
            for (Element link : links) {
                String nextUrl = link.absUrl("href");
                CrawledPage linkedPage = crawlerClient.findByUrl(nextUrl);
                if (linkedPage != null) {
                    int linkedDocId = Math.toIntExact(linkedPage.getId());
                    adjacencyList.computeIfAbsent(docId, k -> new ArrayList<>()).add(linkedDocId);
                }
            }
        }

        // Compute TF-IDF and save to database
        saveTfIdfToDatabase(termFrequencyMap, documentFrequencyMap, pages.size());

        // Compute PageRank and save to database
        Map<Integer, Double> pageRankScores = PageRank.calculate(adjacencyList);
        for (Map.Entry<Integer, Double> entry : pageRankScores.entrySet()) {
            CrawledPage page = crawlerClient.findById((long) entry.getKey()).orElse(null);
            if (page != null) {
                page.setPageRankScore(entry.getValue());
                crawlerClient.save(page);
            }
        }
    }

    private String[] tokenize(String text) {
        return WORD_PATTERN.matcher(text.toLowerCase()).results()
                .map(match -> match.group())
                .toArray(String[]::new);
    }

    private void saveTfIdfToDatabase(Map<String, Map<Integer, Integer>> termFrequencyMap,
                                     Map<String, Integer> documentFrequencyMap, int totalDocuments) {
        for (Map.Entry<String, Map<Integer, Integer>> entry : termFrequencyMap.entrySet()) {
            String word = entry.getKey();
            Map<Integer, Integer> tfMap = entry.getValue();
            int df = documentFrequencyMap.getOrDefault(word, 1);

            Map<Integer, Double> tfidfScores = new HashMap<>();
            for (Map.Entry<Integer, Integer> docEntry : tfMap.entrySet()) {
                int docId = docEntry.getKey();
                int tf = docEntry.getValue();
                double idf = Math.log((double) totalDocuments / df);
                double tfIdf = tf * idf;
                tfidfScores.put(docId, tfIdf);
            }

            // Save TF-IDF scores to database
            InvertedIndex index = invertedIndexRepository.findByWord(word);
            if (index == null) {
                index = InvertedIndex.builder()
                        .word(word)
                        .documentIds(new ArrayList<>(tfidfScores.keySet()))
                        .tfidfScores(tfidfScores)
                        .build();
            } else {
                index.getDocumentIds().addAll(tfidfScores.keySet());
                index.getTfidfScores().putAll(tfidfScores);
            }
            invertedIndexRepository.save(index);
        }
    }

    // private void saveToDatabase(Map<String, Set<Integer>> invertedIndexMap) {
    //     for (Map.Entry<String, Set<Integer>> entry : invertedIndexMap.entrySet()) {
    //         String word = entry.getKey();
    //         List<Integer> documentIds = new ArrayList<>(entry.getValue());

    //         InvertedIndex index = invertedIndexRepository.findByWord(word);
    //         if (index == null) {
    //             index = InvertedIndex.builder()
    //                     .word(word)
    //                     .documentIds(documentIds)
    //                     .build();
    //         } else {
    //             index.getDocumentIds().addAll(documentIds);
    //         }
    //         invertedIndexRepository.save(index);
    //     }
    // }

}

@Getter
@Setter
@Builder
@AllArgsConstructor
@NoArgsConstructor
@Entity(name = "inverted_index")
public class InvertedIndex {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @Column(nullable = false, unique = true)
    private String word;

    @ElementCollection(fetch = FetchType.EAGER)
    @CollectionTable(name = "document_ids", joinColumns = @JoinColumn(name = "inverted_index_id"))
    private List<Integer> documentIds;

    @Type(JsonType.class)
    @Column(columnDefinition = "JSONB")
    @Convert(converter = JsonbMapConverter.class)
    private Map<Integer, Double> tfidfScores;
}

public interface InvertedIndexRepository extends JpaRepository<InvertedIndex, Long> {
    InvertedIndex findByWord(String word);
}

@Getter
@Setter
@AllArgsConstructor
@NoArgsConstructor
@Builder
public class CrawledPage {

    private Long id;

    private String url;

    private String title;

    private String shortContent;

    private String content;

    private LocalDateTime createdDate;

    private List<String> tags;

    private double pageRankScore;
}

@FeignClient(name = "crawler", url = "${application.config.crawler.url}")
public interface CrawlerClient {

    @PostMapping("/findByUrl")
    CrawledPage findByUrl(@RequestBody String url);

    @PostMapping("/save")
    Boolean save(@RequestBody CrawledPage page);

    @GetMapping("/findById/{id}")
    Optional<CrawledPage> findById(@PathVariable(name = "id") long id);

}

@Converter(autoApply = true)
public class JsonbMapConverter implements AttributeConverter<Map<Integer, Double>, String> {

    private static final ObjectMapper objectMapper = new ObjectMapper();

    @Override
    public String convertToDatabaseColumn(Map<Integer, Double> map) {
        try {
            return objectMapper.writeValueAsString(map);  // Convert to JSON string
        } catch (IOException e) {
            throw new RuntimeException("Error converting map to JSON", e);
        }
    }

    @Override
    public Map<Integer, Double> convertToEntityAttribute(String json) {
        try {
            return objectMapper.readValue(json, new TypeReference<Map<Integer, Double>>() {
            });
        } catch (IOException e) {
            throw new RuntimeException("Error converting JSON to map", e);
        }
    }
}

